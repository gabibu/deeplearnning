{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "DL_word_embedding_assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3UhbKu23RsL",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding - Home Assigment\n",
        "## Dr. Omri Allouche 2018. YData Deep Learning Course\n",
        "\n",
        "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_word_embedding_assignment.ipynb)\n",
        "    \n",
        "    \n",
        "In this exercise, you'll use word vectors trained on a corpus of 380,000 lyrics of songs from MetroLyrics (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).  \n",
        "The dataset contains these fields for each song, in CSV format:\n",
        "1. index\n",
        "1. song\n",
        "1. year\n",
        "1. artist\n",
        "1. genre\n",
        "1. lyrics\n",
        "\n",
        "Before doing this exercise, we recommend that you go over the \"Bag of words meets bag of popcorn\" tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
        "\n",
        "Other recommended resources:\n",
        "- https://rare-technologies.com/word2vec-tutorial/\n",
        "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ock7CX_3RsN",
        "colab_type": "text"
      },
      "source": [
        "### Train word vectors\n",
        "Train word vectors using the Skipgram Word2vec algorithm and the gensim package.\n",
        "Make sure you perform the following:\n",
        "- Tokenize words\n",
        "- Lowercase all words\n",
        "- Remove punctuation marks\n",
        "- Remove rare words\n",
        "- Remove stopwords\n",
        "\n",
        "Use 300 as the dimension of the word vectors. Try different context sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JOvvKt4GkRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install livelossplot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhmPqLwI3RsO",
        "colab_type": "code",
        "outputId": "24bd745b-217f-4860-aea1-e6d25ee598b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk \n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "pd.set_option('display.width', -1)\n",
        "pd.set_option('max_colwidth', -1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier,XGBRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "stop_words = set(stopwords.words(\"english\"))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CopC33vU4OtL",
        "colab_type": "code",
        "outputId": "a7e9fe22-455f-4f0d-e2b2-84c2d821170f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJUylIph4Fcs",
        "colab_type": "code",
        "outputId": "f4cfe3b5-d17c-4588-d043-a30ac446a12d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "lyrics_df = pd.read_csv(\"/content/drive/My Drive/lyrics.csv\", usecols=['genre', 'lyrics'])\n",
        "lyrics_df.sample(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genre</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>190771</th>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271327</th>\n",
              "      <td>Rock</td>\n",
              "      <td>Throw me in the water\\n'Cause you know that I could never swim\\nI'll think of you as I go down\\nAnd you put yourself in order\\n'Cause you know that I could never count\\nAs high as you my nose would bleed\\nAnd your heart is like a stone\\nYour flesh has turned to mold\\nDon't feel bad if hell is not the place you hoped it'll be\\nAnd you put aside your dagger\\n'Cause you know that you might need it soon\\nTo cut me open, feel around inside\\nAnd you make like I'm your father\\nSo the blood that spills is all the same\\nTo me and everyone you know\\nYour heart is like a stone\\nAnd your flesh has turned to mold\\nDon't feel bad if hell is not the place you hoped it'll be\\nAnd you say it doesn't matter\\n'Cause you know from all those books you never read\\nYou knew how they would end</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        genre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        lyrics\n",
              "190771  Other  NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
              "271327  Rock   Throw me in the water\\n'Cause you know that I could never swim\\nI'll think of you as I go down\\nAnd you put yourself in order\\n'Cause you know that I could never count\\nAs high as you my nose would bleed\\nAnd your heart is like a stone\\nYour flesh has turned to mold\\nDon't feel bad if hell is not the place you hoped it'll be\\nAnd you put aside your dagger\\n'Cause you know that you might need it soon\\nTo cut me open, feel around inside\\nAnd you make like I'm your father\\nSo the blood that spills is all the same\\nTo me and everyone you know\\nYour heart is like a stone\\nAnd your flesh has turned to mold\\nDon't feel bad if hell is not the place you hoped it'll be\\nAnd you say it doesn't matter\\n'Cause you know from all those books you never read\\nYou knew how they would end"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq_Y2su_7rQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyrics_df = lyrics_df.where((pd.notnull(lyrics_df)), None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWH_CWdc4cpT",
        "colab_type": "code",
        "outputId": "654b76cd-1e14-40dd-c0bb-0d385b561890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "lyrics_df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 362237 entries, 0 to 362236\n",
            "Data columns (total 2 columns):\n",
            "genre     362237 non-null object\n",
            "lyrics    266557 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 5.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJLISvt774kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyrics_df = lyrics_df[lyrics_df['lyrics'].notnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQzH8g-d4Zj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTn-8VWl5quM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanText(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\d', '#', text) \n",
        "  text = re.sub(r\"\\s+\",\" \", text, flags = re.I)\n",
        "  tokens = tokenizer.tokenize(text.lower())\n",
        "  tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "  \n",
        "  return tokens if len(tokens) > 0 else None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmIbguVy5iNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyrics_df['clean_lyrics'] = lyrics_df['lyrics'].apply(cleanText)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU6KyBAy-Mrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyrics_df = lyrics_df[lyrics_df['clean_lyrics'].notnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4SM6U-T68FV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "be94b3ea-9426-43f0-86cf-fd38602c9d5f"
      },
      "source": [
        "lyrics_df[['genre', 'clean_lyrics']].sample(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genre</th>\n",
              "      <th>clean_lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>98211</th>\n",
              "      <td>Pop</td>\n",
              "      <td>[black, colour, true, loves, hair, lips, like, roses, fair, sweetest, smile, gentlest, hands, love, ground, whereon, stands, love, love, well, knows, love, ground, whereon, goes, wish, day, would, soon, come, one, go, clyde, mourn, weep, satisfied, never, sleep, write, letters, short, lines, suffer, death, ten, thousand, times, black, colour, true, loves, hair, lips, like, roses, fair, sweetest, smile, gentlest, hands, love, ground, whereon, stands, love, ground, whereon, stands, love, love, love, ground, whereon, stands]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14348</th>\n",
              "      <td>Rock</td>\n",
              "      <td>[know, gotcha, figured, proud, walkin, tall, whach, whach, whach, make, know, give, take, blame, someone, else, everyone, except, one, step, forward, seems, get, sink, faster, know, want, save, day, grab, reins, run, circles, gonna, crash, drag, world, one, thing, remains, hey, man, aware, seem, day, dont, care, anyway, run, cant, hide, guilt, u, built, inside, forget, never, get, thinking, backwards, hell, get, started, drowning, faster, go, wanna, save, day, grab, reins, run, circles, gonna, crash, drag, world, one, thing, remains, wanna, save, day, grab, reins, run, circles, gonna, crash, drag, world, one, thing, remains]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136701</th>\n",
              "      <td>Country</td>\n",
              "      <td>[tell, wondering, think, youve, gone, mad, speaking, another, language, one, understands, talk, ocean, listen, birds, everyone, around, wonders, heard, youre, broken, youre, beautiful, fit, inside, box, may, seem, blurry, rest, thats, cause, see, colors, one, sees, youre, kaleidoscope, run, chase, sunrise, wish, falling, stars, paint, sky, eyes, hold, moon, arms, youre, broken, youre, beautiful, fit, inside, box, may, seem, blurry, rest, thats, cause, see, colors, one, sees, youre, kaleidoscope, youre, kaleidoscope, instrumental]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          genre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              clean_lyrics\n",
              "98211   Pop      [black, colour, true, loves, hair, lips, like, roses, fair, sweetest, smile, gentlest, hands, love, ground, whereon, stands, love, love, well, knows, love, ground, whereon, goes, wish, day, would, soon, come, one, go, clyde, mourn, weep, satisfied, never, sleep, write, letters, short, lines, suffer, death, ten, thousand, times, black, colour, true, loves, hair, lips, like, roses, fair, sweetest, smile, gentlest, hands, love, ground, whereon, stands, love, ground, whereon, stands, love, love, love, ground, whereon, stands]                                                                                                         \n",
              "14348   Rock     [know, gotcha, figured, proud, walkin, tall, whach, whach, whach, make, know, give, take, blame, someone, else, everyone, except, one, step, forward, seems, get, sink, faster, know, want, save, day, grab, reins, run, circles, gonna, crash, drag, world, one, thing, remains, hey, man, aware, seem, day, dont, care, anyway, run, cant, hide, guilt, u, built, inside, forget, never, get, thinking, backwards, hell, get, started, drowning, faster, go, wanna, save, day, grab, reins, run, circles, gonna, crash, drag, world, one, thing, remains, wanna, save, day, grab, reins, run, circles, gonna, crash, drag, world, one, thing, remains]\n",
              "136701  Country  [tell, wondering, think, youve, gone, mad, speaking, another, language, one, understands, talk, ocean, listen, birds, everyone, around, wonders, heard, youre, broken, youre, beautiful, fit, inside, box, may, seem, blurry, rest, thats, cause, see, colors, one, sees, youre, kaleidoscope, run, chase, sunrise, wish, falling, stars, paint, sky, eyes, hold, moon, arms, youre, broken, youre, beautiful, fit, inside, box, may, seem, blurry, rest, thats, cause, see, colors, one, sees, youre, kaleidoscope, youre, kaleidoscope, instrumental]                                                                                                 "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VziQL7mX4qsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FEATURE_VECTOR_SIZE = 300 \n",
        "WINDOW_SIZE = 5\n",
        "all_sentenses = lyrics_df[\"clean_lyrics\"].tolist()\n",
        "wevec_model = Word2Vec(all_sentenses, size=FEATURE_VECTOR_SIZE, window=WINDOW_SIZE, workers=4, min_count=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za296Eg93RsQ",
        "colab_type": "text"
      },
      "source": [
        "### Review most similar words\n",
        "Get initial evaluation of the word vectors by analyzing the most similar words for a few interesting words in the text. \n",
        "\n",
        "Choose words yourself, and find the most similar words to them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2DbKOJb3RsQ",
        "colab_type": "code",
        "outputId": "bff26398-580c-4b60-b406-df79572010b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "for word in ['love', 'man', 'girl', 'feel']:\n",
        "  print('**********{}**************'.format(word))\n",
        "  print(wevec_model.wv.most_similar('love'))\n",
        "  print('**********{}**************'.format(word))\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********love**************\n",
            "[('know', 0.6976536512374878), ('baby', 0.6739638447761536), ('cause', 0.6532072424888611), ('true', 0.620837926864624), ('oh', 0.6157742738723755), ('want', 0.6149493455886841), ('never', 0.6118445992469788), ('always', 0.6080036163330078), ('heart', 0.6009585857391357), ('way', 0.5991636514663696)]\n",
            "**********love**************\n",
            "**********man**************\n",
            "[('know', 0.6976536512374878), ('baby', 0.6739638447761536), ('cause', 0.6532072424888611), ('true', 0.620837926864624), ('oh', 0.6157742738723755), ('want', 0.6149493455886841), ('never', 0.6118445992469788), ('always', 0.6080036163330078), ('heart', 0.6009585857391357), ('way', 0.5991636514663696)]\n",
            "**********man**************\n",
            "**********girl**************\n",
            "[('know', 0.6976536512374878), ('baby', 0.6739638447761536), ('cause', 0.6532072424888611), ('true', 0.620837926864624), ('oh', 0.6157742738723755), ('want', 0.6149493455886841), ('never', 0.6118445992469788), ('always', 0.6080036163330078), ('heart', 0.6009585857391357), ('way', 0.5991636514663696)]\n",
            "**********girl**************\n",
            "**********feel**************\n",
            "[('know', 0.6976536512374878), ('baby', 0.6739638447761536), ('cause', 0.6532072424888611), ('true', 0.620837926864624), ('oh', 0.6157742738723755), ('want', 0.6149493455886841), ('never', 0.6118445992469788), ('always', 0.6080036163330078), ('heart', 0.6009585857391357), ('way', 0.5991636514663696)]\n",
            "**********feel**************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYRBpD303RsT",
        "colab_type": "text"
      },
      "source": [
        "### Word Vectors Algebra\n",
        "We've seen in class examples of algebraic games on the word vectors (e.g. man - woman + king = queen ). \n",
        "\n",
        "Try a few vector algebra terms, and evaluate how well they work. Try to use the Cosine distance and compare it to the Euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYsRL2eZ3RsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "27437216-eb7d-499c-dfc7-891ebf4921a5"
      },
      "source": [
        "v1 = wevec_model.wv['king'] \n",
        "v2 = wevec_model.wv['queen'] \n",
        "\n",
        "res =  v1 - v2 \n",
        "wevec_model.wv.similar_by_vector(res)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.4416906237602234),\n",
              " ('victorious', 0.28422659635543823),\n",
              " ('christ', 0.28422001004219055),\n",
              " ('overthrow', 0.2781832814216614),\n",
              " ('disarmament', 0.2606436610221863),\n",
              " ('undisputed', 0.2583610415458679),\n",
              " ('overcome', 0.25667399168014526),\n",
              " ('solomon', 0.2539622485637665),\n",
              " ('dispear', 0.24596869945526123),\n",
              " ('battle', 0.23669087886810303)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SnVAt9ZC2e4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "595068f7-d5a3-4d2f-9f5d-518923935dd4"
      },
      "source": [
        "v1 = wevec_model.wv['woman'] \n",
        "v2 = wevec_model.wv['girl'] \n",
        "\n",
        "res =  v1 - v2 \n",
        "wevec_model.wv.similar_by_vector(res)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.3945504426956177),\n",
              " ('witchy', 0.3829672932624817),\n",
              " ('adherence', 0.3591882586479187),\n",
              " ('hath', 0.355106920003891),\n",
              " ('wastelands', 0.35092365741729736),\n",
              " ('hinguckt', 0.3403870761394501),\n",
              " ('wanhopige', 0.3390408158302307),\n",
              " ('incarnate', 0.33836591243743896),\n",
              " ('parachutist', 0.3364051580429077),\n",
              " ('dissatisfied', 0.3338039517402649)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaBATHr0C-xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "95d4ea0b-f4a3-4610-dfc0-18ae63b94ba9"
      },
      "source": [
        "v1 = wevec_model.wv['girl'] \n",
        "v2 = wevec_model.wv['man'] \n",
        "\n",
        "res =  v1 + v2 \n",
        "wevec_model.wv.similar_by_vector(res)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('girl', 0.8658943176269531),\n",
              " ('man', 0.8452440500259399),\n",
              " ('woman', 0.7247971892356873),\n",
              " ('boy', 0.7180936336517334),\n",
              " ('know', 0.6527124047279358),\n",
              " ('baby', 0.6476815938949585),\n",
              " ('cause', 0.638348400592804),\n",
              " ('got', 0.6022270321846008),\n",
              " ('yeah', 0.5943912267684937),\n",
              " ('love', 0.5738491415977478)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhR4V2nNDJw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "ecf899a6-83a2-4991-b6d7-1f323640f704"
      },
      "source": [
        "v1 = wevec_model.wv['woman'] \n",
        "v2 = wevec_model.wv['husband'] \n",
        "\n",
        "res =  v1 + v2 \n",
        "wevec_model.wv.similar_by_vector(res)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.8930797576904297),\n",
              " ('husband', 0.7790439128875732),\n",
              " ('wife', 0.6306020021438599),\n",
              " ('man', 0.6067594289779663),\n",
              " ('girl', 0.5493606328964233),\n",
              " ('daughter', 0.5384731292724609),\n",
              " ('child', 0.49661391973495483),\n",
              " ('women', 0.48693349957466125),\n",
              " ('neighbor', 0.46220311522483826),\n",
              " ('lovin', 0.4582667052745819)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IKrj51iDQc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2b75d86e-f623-4bb2-e0b5-6dd4dfaf717f"
      },
      "source": [
        "v1 = wevec_model.wv['wife'] \n",
        "v2 = wevec_model.wv['woman'] \n",
        "\n",
        "res =  v1 - v2 \n",
        "wevec_model.wv.similar_by_vector(res)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wife', 0.5940633416175842),\n",
              " ('doorjust', 0.39944425225257874),\n",
              " ('beaters', 0.38050615787506104),\n",
              " ('tillman', 0.3772699236869812),\n",
              " ('nephews', 0.3584328591823578),\n",
              " ('ahhhhaaaaaaaaghhhh', 0.3539944887161255),\n",
              " ('attended', 0.34795355796813965),\n",
              " ('swipe', 0.33600378036499023),\n",
              " ('saver', 0.3341713845729828),\n",
              " ('burglary', 0.32993119955062866)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-gKSCMdDejf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0a023c33-4700-496b-907b-13ff71e2d866"
      },
      "source": [
        "v1 = wevec_model.wv['young'] \n",
        "v2 = wevec_model.wv['baby'] \n",
        "\n",
        "res =  v1 + v2 \n",
        "wevec_model.wv.similar_by_vector(res)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('young', 0.7975558042526245),\n",
              " ('baby', 0.7816184163093567),\n",
              " ('girl', 0.6593176126480103),\n",
              " ('love', 0.6478465795516968),\n",
              " ('cause', 0.6436699628829956),\n",
              " ('know', 0.6418748497962952),\n",
              " ('yeah', 0.6136243343353271),\n",
              " ('boy', 0.6044150590896606),\n",
              " ('got', 0.5908496975898743),\n",
              " ('want', 0.58655846118927)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_CwD9Fk3RsV",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis\n",
        "Estimate sentiment of words using word vectors.  \n",
        "In this section, we'll use the SemEval-2015 English Twitter Sentiment Lexicon.  \n",
        "The lexicon was used as an official test set in the SemEval-2015 shared Task #10: Subtask E, and contains a polarity score for words in range -1 (negative) to 1 (positive) - http://saifmohammad.com/WebPages/SCL.html#OPP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiBP0tHv3RsV",
        "colab_type": "text"
      },
      "source": [
        "Build a classifier for the sentiment of a word given its word vector. Split the data to a train and test sets, and report the model performance on both sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vggDl4sv3RsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "1df2ef13-78af-430a-db80-05ea2827f56c"
      },
      "source": [
        "twitter_lex_df = pd.read_csv('/content/drive/My Drive/SemEval2015-English-Twitter-Lexicon.txt', sep='\\t',  names =['polarity', 'word'])\n",
        "twitter_lex_df.describe(include = 'all')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1515.000000</td>\n",
              "      <td>1515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>giveaway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.000319</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.502247</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.984000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.422000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.031000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.406000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.984000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           polarity      word\n",
              "count   1515.000000  1515    \n",
              "unique NaN           1515    \n",
              "top    NaN           giveaway\n",
              "freq   NaN           1       \n",
              "mean    0.000319     NaN     \n",
              "std     0.502247     NaN     \n",
              "min    -0.984000     NaN     \n",
              "25%    -0.422000     NaN     \n",
              "50%     0.031000     NaN     \n",
              "75%     0.406000     NaN     \n",
              "max     0.984000     NaN     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-U69mo7D1_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "fc893da1-e448-4908-8c8c-186820bc4286"
      },
      "source": [
        "twitter_lex_df.sample(3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>0.391</td>\n",
              "      <td>no rush</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>0.188</td>\n",
              "      <td>read</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>0.391</td>\n",
              "      <td>proper</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     polarity     word\n",
              "382  0.391     no rush\n",
              "584  0.188     read   \n",
              "390  0.391     proper "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFjvMj3CD7dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_lex_df['vector'] = twitter_lex_df[\"word\"].apply(lambda word: wevec_model.wv[word].reshape(1,-1) if word in wevec_model.wv else None)\n",
        "twitter_lex_df = twitter_lex_df[twitter_lex_df.vector.notnull()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OkGlggnEB_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \n",
        "train, test = train_test_split(twitter_lex_df, test_size=0.2, random_state=999)\n",
        "X_train = train['vector'].values\n",
        "y_train = train['polarity'].values\n",
        "X_test = test['vector'].values\n",
        "y_test = test['polarity'].values\n",
        "\n",
        "X_train = np.concatenate(X_train).reshape(X_train.shape[0], X_train[0].shape[1])\n",
        "\n",
        "shape = list(X_test[0].shape)\n",
        "X_test = np.concatenate(X_test).reshape(X_test.shape[0],X_test[0].shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPm1o54AEU5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "50efea12-5fd7-4cc8-b5ce-a0419b147873"
      },
      "source": [
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15:24:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
              "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "             silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4gqsIBsEYIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8fada3d-9a21-4fad-908c-def9c847d986"
      },
      "source": [
        "model.score(X_test, y_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35175120519967285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZpF0HWqE-7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05dab398-9744-4865-c5c2-713ce4c8a28b"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "use_cuda"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BjHOvWyFFzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as utils_data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "training_samples = utils_data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "data_loader = utils_data.DataLoader(training_samples, batch_size=5, shuffle=False)\t\n",
        "\n",
        "test_samples = utils_data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "test_data_loader = utils_data.DataLoader(test_samples, batch_size=len(X_test), shuffle=False)\t\n",
        "test_data_loader_1 = utils_data.DataLoader(test_samples, batch_size=1, shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0OCCruLEsR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8537317-b4cd-4e31-d9a8-2cbe42cab792"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
        "        self.dropout1 = nn.Dropout(p=0.1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))   \n",
        "        x = self.dropout1(x)\n",
        "        x = self.predict(x)  \n",
        "        return x\n",
        "\n",
        "      \n",
        "      \n",
        "net = Net(n_feature=300, n_hidden=20, n_output=1)     \n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "if use_cuda:\n",
        "  net = net.cuda()\n",
        "  \n",
        "# plt.ion()  \n",
        "\n",
        "for epoch in range(1000):\n",
        "  \n",
        "  batchloss = 0.0\n",
        "  for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        \n",
        "    if use_cuda:\n",
        "      data = data.cuda()\n",
        "      target = target.cuda()\n",
        "          \n",
        "          \n",
        "    data, target = Variable(data).float(), Variable(target).float()\n",
        "    optimizer.zero_grad()\n",
        "    prediction = net(data)\n",
        "    loss = loss_func(prediction, target) \n",
        "        \n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "    \n",
        "    batchloss += loss.item()\n",
        "  \n",
        "  print(batchloss)\n",
        "        \n",
        "\n",
        "def score(model, data_loader, criterion):\n",
        "  \n",
        "  running_loss = 0.\n",
        "  calc_count =0.\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for images, labels in data_loader:\n",
        "      if use_cuda:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        \n",
        "      images = Variable(images)  \n",
        "      labels = Variable(labels)\n",
        "      \n",
        "      outputs = model(images.float())\n",
        "      loss = criterion(outputs, labels.float())\n",
        "      return loss \n",
        "\n",
        "score(net, test_data_loader, loss_func ).item()  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35.32020165209542\n",
            "34.12065568752587\n",
            "33.36858466826379\n",
            "32.861710192635655\n",
            "32.46283303387463\n",
            "32.039268956985325\n",
            "31.791825458873063\n",
            "31.47485022060573\n",
            "31.27802703413181\n",
            "30.98533385974588\n",
            "30.876199471298605\n",
            "30.97434275317937\n",
            "30.450614749686792\n",
            "30.566897857235745\n",
            "30.258383152307943\n",
            "30.269483193871565\n",
            "30.196752968011424\n",
            "30.2268483436128\n",
            "30.121287770103663\n",
            "29.820330894202925\n",
            "29.84012677008286\n",
            "29.766415531401435\n",
            "29.723762664943933\n",
            "29.81244556279853\n",
            "29.72858191933483\n",
            "29.68568113958463\n",
            "29.509966120764147\n",
            "29.457513962493977\n",
            "29.521088231587783\n",
            "29.58354257029714\n",
            "29.415298566178535\n",
            "29.479346899315715\n",
            "29.40049005218316\n",
            "29.37178146687802\n",
            "29.313269121154008\n",
            "29.359418812392505\n",
            "29.20186443568673\n",
            "29.282623132690787\n",
            "29.193385927175996\n",
            "29.184797887261993\n",
            "29.193913496957975\n",
            "29.18273541907547\n",
            "29.03546228356754\n",
            "28.985123107981963\n",
            "29.01842256053351\n",
            "29.123007585254527\n",
            "29.02714754010958\n",
            "28.907948882551864\n",
            "29.068279995030025\n",
            "28.853292530773615\n",
            "28.87027514021611\n",
            "28.961691731587052\n",
            "28.832644348680333\n",
            "28.988779309831443\n",
            "28.9865377034439\n",
            "28.83983718291711\n",
            "28.769884115899913\n",
            "28.825631704181433\n",
            "28.857210348456647\n",
            "28.8100441894494\n",
            "28.609592676890315\n",
            "28.757653609616682\n",
            "28.877080989899696\n",
            "28.68088433938101\n",
            "28.657606719880278\n",
            "28.713948140619323\n",
            "28.56421490007233\n",
            "28.584903127280995\n",
            "28.6377255773441\n",
            "28.558092500737985\n",
            "28.48733622002237\n",
            "28.49062492712983\n",
            "28.62435484060552\n",
            "28.585801075329073\n",
            "28.542579782773828\n",
            "28.564054332988235\n",
            "28.484292961959\n",
            "28.533879705739764\n",
            "28.515344294922215\n",
            "28.394809839310255\n",
            "28.431098608940374\n",
            "28.37722215242502\n",
            "28.459809944398557\n",
            "28.41688019143737\n",
            "28.45295803320002\n",
            "28.407778931327357\n",
            "28.42652659542091\n",
            "28.380443117581308\n",
            "28.313274845032367\n",
            "28.252043122206487\n",
            "28.247130782343447\n",
            "28.294240896590054\n",
            "28.307859705927456\n",
            "28.32814927434967\n",
            "28.262363098428523\n",
            "28.225066177686585\n",
            "28.2415519651895\n",
            "28.247105173076534\n",
            "28.17349274456501\n",
            "28.297879663785352\n",
            "28.223836230114102\n",
            "28.157676912379657\n",
            "28.24320817552507\n",
            "28.26511731557548\n",
            "28.149951397208497\n",
            "28.14539207292546\n",
            "28.092979694622045\n",
            "28.19330100658408\n",
            "28.055803116149036\n",
            "27.99076866917312\n",
            "28.016057305023423\n",
            "28.122093217651127\n",
            "28.027051782235503\n",
            "28.075710898265243\n",
            "28.096636114176363\n",
            "28.07351103326073\n",
            "28.01587247075986\n",
            "28.068868721424224\n",
            "28.088264413177967\n",
            "27.970945495180786\n",
            "27.89921664724534\n",
            "27.908795134571847\n",
            "27.98127160128206\n",
            "27.917265661992133\n",
            "27.92224466119069\n",
            "27.874912557341304\n",
            "27.87131492431763\n",
            "27.999374375212938\n",
            "27.84390199248446\n",
            "27.831326933403034\n",
            "27.903522460199383\n",
            "27.895598911638444\n",
            "27.886047648753447\n",
            "27.813227364529666\n",
            "27.840277984061686\n",
            "27.831954683642834\n",
            "27.694480963597016\n",
            "27.87403847826613\n",
            "27.894959269207902\n",
            "27.679550374290102\n",
            "27.86048395594662\n",
            "27.766837331466377\n",
            "27.75144774047658\n",
            "27.700336729642004\n",
            "27.757720364501438\n",
            "27.79998262040317\n",
            "27.75639809668064\n",
            "27.72633775511349\n",
            "27.64728985633701\n",
            "27.741588280070573\n",
            "27.67056575830793\n",
            "27.85896821662027\n",
            "27.740806271584006\n",
            "27.698023973022828\n",
            "27.605716845733696\n",
            "27.652030352677684\n",
            "27.633952232841693\n",
            "27.5095352477947\n",
            "27.64261870966584\n",
            "27.59068475633103\n",
            "27.55676782410592\n",
            "27.74024795833975\n",
            "27.611648600868648\n",
            "27.731530791614205\n",
            "27.634315616363892\n",
            "27.606504366733134\n",
            "27.573121565190377\n",
            "27.549070692546593\n",
            "27.499201362021267\n",
            "27.439276153437504\n",
            "27.596121532842517\n",
            "27.54633426084183\n",
            "27.55103448263253\n",
            "27.5720595765888\n",
            "27.682821564842016\n",
            "27.528771275479812\n",
            "27.485180923900664\n",
            "27.598339941170707\n",
            "27.463441664353013\n",
            "27.537686806273996\n",
            "27.48649948759703\n",
            "27.432303928304464\n",
            "27.43204811378382\n",
            "27.45296390641306\n",
            "27.41825218067561\n",
            "27.485303095862037\n",
            "27.521313033992897\n",
            "27.365337897092104\n",
            "27.426292853429914\n",
            "27.47907807516185\n",
            "27.431069744750857\n",
            "27.456425966358438\n",
            "27.33819861803795\n",
            "27.38455233846298\n",
            "27.44927849661508\n",
            "27.393434081284795\n",
            "27.394463529462996\n",
            "27.38290567639342\n",
            "27.342314857106807\n",
            "27.4046756041937\n",
            "27.35695741287782\n",
            "27.353509843509528\n",
            "27.439798810584307\n",
            "27.31299593548465\n",
            "27.303353016321125\n",
            "27.330877235845037\n",
            "27.293439123546705\n",
            "27.261151389011502\n",
            "27.244323653168976\n",
            "27.296933218836784\n",
            "27.363061303272843\n",
            "27.29635072563542\n",
            "27.31588586224825\n",
            "27.301704593468457\n",
            "27.222034332342446\n",
            "27.295436439453624\n",
            "27.272819839883596\n",
            "27.313436210810323\n",
            "27.38382307170832\n",
            "27.294952268712223\n",
            "27.30465825420106\n",
            "27.271728400868597\n",
            "27.210708517130115\n",
            "27.23146528977668\n",
            "27.25554125160852\n",
            "27.24742676002643\n",
            "27.169966484972974\n",
            "27.193023957674086\n",
            "27.228515485301614\n",
            "27.163983714326605\n",
            "27.216706861232524\n",
            "27.246664086027522\n",
            "27.116797840222716\n",
            "27.222723587357905\n",
            "27.19979532272555\n",
            "27.242310451809317\n",
            "27.19546987709691\n",
            "27.150980375216022\n",
            "27.049956197703295\n",
            "27.071329174301354\n",
            "27.148521779105067\n",
            "27.151350275729783\n",
            "27.1053108262131\n",
            "27.07738843653351\n",
            "27.020045987588674\n",
            "27.032713281252654\n",
            "27.09615813801065\n",
            "27.127288745999977\n",
            "27.11083627025073\n",
            "27.106515081111866\n",
            "27.032154398038983\n",
            "27.09899596730247\n",
            "27.113182315209997\n",
            "27.057159154574492\n",
            "27.05911497119814\n",
            "27.05836076196283\n",
            "27.019068730063736\n",
            "27.136853427458163\n",
            "27.032347766688872\n",
            "27.064235213830216\n",
            "26.929661423775542\n",
            "26.98094160320761\n",
            "27.0584999921266\n",
            "27.06087218892668\n",
            "27.094914292916656\n",
            "26.938664329778476\n",
            "27.190387703478336\n",
            "27.12185086501995\n",
            "27.112127175554633\n",
            "26.94116944313282\n",
            "27.01062491349876\n",
            "27.028473232443503\n",
            "27.06557977735065\n",
            "26.98696157496306\n",
            "26.999381051104137\n",
            "27.070704084821045\n",
            "27.001004018059007\n",
            "27.050302607691492\n",
            "26.99735414213501\n",
            "27.022788337777456\n",
            "26.92365299258381\n",
            "26.875992876826786\n",
            "26.962294454686344\n",
            "27.033424170629587\n",
            "26.852389206338557\n",
            "26.943879928687238\n",
            "27.029081226093695\n",
            "26.88434712064918\n",
            "26.94577785814181\n",
            "26.90027073584497\n",
            "27.060549079086684\n",
            "26.934770475141704\n",
            "26.94832985533867\n",
            "26.847648942668457\n",
            "26.92874043793563\n",
            "26.817749367684883\n",
            "26.913091713277936\n",
            "26.93596606806932\n",
            "26.98749370314181\n",
            "26.936389732392854\n",
            "26.8535412279889\n",
            "26.880997249610118\n",
            "26.8245237915462\n",
            "26.83545224231125\n",
            "26.893931813770905\n",
            "26.898303371854126\n",
            "26.822538222870207\n",
            "26.90874597922084\n",
            "26.862434375640987\n",
            "26.799519731534588\n",
            "26.938118528065388\n",
            "26.84423347059237\n",
            "26.790889380520184\n",
            "26.888243595836684\n",
            "26.762735084048472\n",
            "26.94801526525407\n",
            "26.809348008944653\n",
            "26.832982131745666\n",
            "26.791577874915674\n",
            "26.771704532104195\n",
            "26.717777239435236\n",
            "26.82854719478928\n",
            "26.81426022416781\n",
            "26.862650784412835\n",
            "26.83205635810009\n",
            "26.811330173657552\n",
            "26.812504351546522\n",
            "26.787266932427883\n",
            "26.80369699226867\n",
            "26.88141741603613\n",
            "26.769639137084596\n",
            "26.78882615083421\n",
            "26.84669552530977\n",
            "26.783399511725293\n",
            "26.791600103199016\n",
            "26.88095928114626\n",
            "26.747005722601898\n",
            "26.834425698965788\n",
            "26.69541941722855\n",
            "26.852674068999477\n",
            "26.75000280828681\n",
            "26.820859811996343\n",
            "26.71253801707644\n",
            "26.774032420420554\n",
            "26.827819842495956\n",
            "26.759613731395802\n",
            "26.633635447469715\n",
            "26.705264792661183\n",
            "26.630068969825516\n",
            "26.703187195285864\n",
            "26.656738647900056\n",
            "26.798786834016028\n",
            "26.724135482440033\n",
            "26.7984040862857\n",
            "26.64773395405635\n",
            "26.6953085078855\n",
            "26.636213740368476\n",
            "26.761888956325492\n",
            "26.69556962745264\n",
            "26.53693975898966\n",
            "26.676543911918998\n",
            "26.74942421112155\n",
            "26.674476838539704\n",
            "26.703176638111472\n",
            "26.697341138496995\n",
            "26.645374413652462\n",
            "26.699053209842532\n",
            "26.692432897121762\n",
            "26.64390234905295\n",
            "26.603546899797948\n",
            "26.626348969512037\n",
            "26.812186820723582\n",
            "26.607506687752903\n",
            "26.71543213352561\n",
            "26.593444161117077\n",
            "26.652176314761164\n",
            "26.525787575756112\n",
            "26.589902078139858\n",
            "26.61962255940307\n",
            "26.707018739962223\n",
            "26.540203356130405\n",
            "26.60468150375891\n",
            "26.681169087300077\n",
            "26.626233896240592\n",
            "26.519809379708022\n",
            "26.627502445731807\n",
            "26.544208083301783\n",
            "26.622932335147198\n",
            "26.587424736890682\n",
            "26.646597979590297\n",
            "26.56070869024552\n",
            "26.455410328497237\n",
            "26.535045044876313\n",
            "26.571514126379043\n",
            "26.54254954389762\n",
            "26.603003478914616\n",
            "26.61559715551266\n",
            "26.61585355513307\n",
            "26.605950423676404\n",
            "26.54270135314073\n",
            "26.646542170085013\n",
            "26.47528358717682\n",
            "26.588105203642044\n",
            "26.53914889130101\n",
            "26.560419356363127\n",
            "26.545960951174493\n",
            "26.55241554323584\n",
            "26.47800939111039\n",
            "26.568187640048563\n",
            "26.628822335842415\n",
            "26.51886205561459\n",
            "26.567679587053135\n",
            "26.651754081867693\n",
            "26.58681827811597\n",
            "26.510927667459327\n",
            "26.455223651370034\n",
            "26.62760111226089\n",
            "26.462932514652493\n",
            "26.51859077543486\n",
            "26.484663393406663\n",
            "26.518789058383845\n",
            "26.51133929442858\n",
            "26.489371767838065\n",
            "26.60552751738578\n",
            "26.65660978667438\n",
            "26.460614079156585\n",
            "26.54284063022078\n",
            "26.514714080934255\n",
            "26.557937991715335\n",
            "26.477442683529965\n",
            "26.58300714118377\n",
            "26.52546793880174\n",
            "26.473657018391805\n",
            "26.5101679507643\n",
            "26.511137136491016\n",
            "26.52055393935734\n",
            "26.578412914997898\n",
            "26.47590979275992\n",
            "26.39718688663288\n",
            "26.47253354064287\n",
            "26.481167123098203\n",
            "26.459829945346428\n",
            "26.430860955773824\n",
            "26.505317056085914\n",
            "26.553175224747974\n",
            "26.45689547143411\n",
            "26.43043358932482\n",
            "26.43130392767489\n",
            "26.458560956956717\n",
            "26.62170396395959\n",
            "26.484323049502564\n",
            "26.422622163488995\n",
            "26.462519209482707\n",
            "26.549073453526944\n",
            "26.561375339748338\n",
            "26.501938667453942\n",
            "26.400686309221783\n",
            "26.479380182725436\n",
            "26.44267919594131\n",
            "26.391125499736518\n",
            "26.4341071107483\n",
            "26.540684541512746\n",
            "26.49604923314473\n",
            "26.42733090017282\n",
            "26.383553764710086\n",
            "26.3574168519699\n",
            "26.410709628557015\n",
            "26.421654463279992\n",
            "26.4777836815083\n",
            "26.373930368572474\n",
            "26.423642111214576\n",
            "26.460149545688182\n",
            "26.435432869766373\n",
            "26.43874582307035\n",
            "26.524258615158033\n",
            "26.40830203704536\n",
            "26.413606160203926\n",
            "26.43104169293656\n",
            "26.38516861665994\n",
            "26.371335724528763\n",
            "26.464384801824053\n",
            "26.480213166680187\n",
            "26.517653602932114\n",
            "26.381149454973638\n",
            "26.441773342721717\n",
            "26.455626951530576\n",
            "26.485901016341813\n",
            "26.45647103599913\n",
            "26.47181484475732\n",
            "26.357607393030776\n",
            "26.402195824543014\n",
            "26.380119934445247\n",
            "26.313623830210418\n",
            "26.39254320471082\n",
            "26.49864893578342\n",
            "26.292665175024013\n",
            "26.291740878263226\n",
            "26.421887620767848\n",
            "26.390596725438627\n",
            "26.348605510313007\n",
            "26.373387342313947\n",
            "26.450500490144805\n",
            "26.37020992167527\n",
            "26.306867627588588\n",
            "26.481583777961532\n",
            "26.45467339717652\n",
            "26.354669576510787\n",
            "26.406411399024364\n",
            "26.295925255923066\n",
            "26.342362455781313\n",
            "26.392890629345857\n",
            "26.34011534830387\n",
            "26.26195651640319\n",
            "26.359670891532005\n",
            "26.40154416114092\n",
            "26.35486621549353\n",
            "26.356327081783093\n",
            "26.2390751915882\n",
            "26.396067648922326\n",
            "26.382380275346804\n",
            "26.39520924910903\n",
            "26.321170026174514\n",
            "26.375307437381707\n",
            "26.35653429903323\n",
            "26.391059671421772\n",
            "26.38792611286044\n",
            "26.38203265797347\n",
            "26.336842468124814\n",
            "26.31374902994139\n",
            "26.42362840194255\n",
            "26.353526385879377\n",
            "26.311184335500002\n",
            "26.3393461638193\n",
            "26.258990846509732\n",
            "26.293875755101908\n",
            "26.262158853933215\n",
            "26.375613286421867\n",
            "26.405538468476152\n",
            "26.45854381704703\n",
            "26.29447715999777\n",
            "26.29853615828324\n",
            "26.404667447670363\n",
            "26.240400994895026\n",
            "26.263720432383707\n",
            "26.314314831795855\n",
            "26.34120644108043\n",
            "26.34649874950628\n",
            "26.196959903318202\n",
            "26.307254377752542\n",
            "26.293642258271575\n",
            "26.307328815246365\n",
            "26.2827309332788\n",
            "26.237163836107356\n",
            "26.341062527149916\n",
            "26.280640778613417\n",
            "26.282028289162554\n",
            "26.267850228236057\n",
            "26.34177127861767\n",
            "26.161180113758746\n",
            "26.330971520394087\n",
            "26.264608848665375\n",
            "26.266537703049835\n",
            "26.273896727954707\n",
            "26.255585202288785\n",
            "26.318144832421694\n",
            "26.300798382610083\n",
            "26.34385573863983\n",
            "26.382950350642204\n",
            "26.255955801694654\n",
            "26.338087020089006\n",
            "26.25413268781267\n",
            "26.301720532006584\n",
            "26.281018879439216\n",
            "26.22886022180319\n",
            "26.251068034776836\n",
            "26.25228318484733\n",
            "26.28210946219042\n",
            "26.309661409715773\n",
            "26.219560277084383\n",
            "26.26275090501622\n",
            "26.299788573756814\n",
            "26.178880175109953\n",
            "26.27367292152485\n",
            "26.2180811855942\n",
            "26.32389151494135\n",
            "26.189431591366883\n",
            "26.262690773699433\n",
            "26.189701685172622\n",
            "26.286368152588693\n",
            "26.269455921647022\n",
            "26.26364178283984\n",
            "26.27552017392327\n",
            "26.217029090668802\n",
            "26.258106573484838\n",
            "26.307037119942834\n",
            "26.18764462391846\n",
            "26.20426002703607\n",
            "26.220787025493337\n",
            "26.1621116642782\n",
            "26.218362780826283\n",
            "26.20516042521922\n",
            "26.16772773861885\n",
            "26.228787084575742\n",
            "26.170306755928323\n",
            "26.2980692956844\n",
            "26.191492043901235\n",
            "26.186401898295856\n",
            "26.218029697503425\n",
            "26.288496887166048\n",
            "26.20194260825407\n",
            "26.262646318972656\n",
            "26.161243547685444\n",
            "26.152911523773582\n",
            "26.116413832649414\n",
            "26.164420446380973\n",
            "26.173783946287585\n",
            "26.146895962787767\n",
            "26.14140499121538\n",
            "26.18232842045836\n",
            "26.228374749422073\n",
            "26.202008229098283\n",
            "26.159950295463204\n",
            "26.147218704223633\n",
            "26.144952929271312\n",
            "26.120218700089026\n",
            "26.20315469884008\n",
            "26.15978204619023\n",
            "26.216879438608885\n",
            "26.055874062352814\n",
            "26.195064020284917\n",
            "26.18780976557173\n",
            "26.10963172855554\n",
            "26.245774436509237\n",
            "26.14415982700666\n",
            "26.12031744091655\n",
            "26.17174132168293\n",
            "26.13762731164308\n",
            "26.22367598731944\n",
            "26.114175971698614\n",
            "26.28106472549439\n",
            "26.083191767625976\n",
            "26.03823897176335\n",
            "26.206290986134263\n",
            "26.16184477168099\n",
            "26.172425497356926\n",
            "26.074752339045517\n",
            "26.20634717198891\n",
            "26.199315629317425\n",
            "26.18042340624379\n",
            "26.17558247666966\n",
            "26.184953705218504\n",
            "26.128076282384427\n",
            "26.12353476298813\n",
            "26.185349726622007\n",
            "26.06328571985796\n",
            "26.139696706764425\n",
            "26.103945663366176\n",
            "26.124739103019238\n",
            "26.16964313882636\n",
            "26.054572346160057\n",
            "26.15265232132515\n",
            "26.07428645813343\n",
            "26.151189465017524\n",
            "26.12151425140837\n",
            "26.227148636244237\n",
            "26.21006828499958\n",
            "26.077781641855836\n",
            "26.19371829496231\n",
            "26.121543174609542\n",
            "26.132843753006455\n",
            "26.176011097151786\n",
            "26.086396508835605\n",
            "26.04963871347718\n",
            "26.050284366941924\n",
            "26.089869288902264\n",
            "26.179881747520994\n",
            "26.128630279330537\n",
            "26.08185894007329\n",
            "26.136067708954215\n",
            "26.172417446272448\n",
            "26.166801595536526\n",
            "26.132476666709408\n",
            "26.083443051938957\n",
            "26.180776791938115\n",
            "26.196589525556192\n",
            "26.135822302829183\n",
            "26.13978600577684\n",
            "26.04337063524872\n",
            "26.101715087657794\n",
            "26.173785517457873\n",
            "26.11464076768607\n",
            "26.128081299597397\n",
            "26.114183768397197\n",
            "26.100665066885995\n",
            "26.05899429821875\n",
            "26.05812545779827\n",
            "25.954758322695852\n",
            "26.068057227879763\n",
            "26.050479542289395\n",
            "26.08268059854163\n",
            "26.13789850845933\n",
            "26.052703050139826\n",
            "26.080530903302133\n",
            "26.076025055142964\n",
            "26.03886738975416\n",
            "26.080029043136165\n",
            "26.132100109123144\n",
            "25.973267729685176\n",
            "26.18852474441519\n",
            "26.069216455813148\n",
            "26.09379015554441\n",
            "26.031655095750466\n",
            "26.019516049815138\n",
            "26.04732423288806\n",
            "26.144727921346202\n",
            "26.030687144325896\n",
            "26.05522987282211\n",
            "26.06318679580636\n",
            "25.99456882231948\n",
            "26.071391878708454\n",
            "25.987196343414325\n",
            "26.103898057361448\n",
            "26.072360250502243\n",
            "26.118604157616574\n",
            "26.122278718763482\n",
            "26.139537000097334\n",
            "26.03394160635071\n",
            "26.1520824090112\n",
            "26.043880461384106\n",
            "26.038477916670672\n",
            "25.984343292475387\n",
            "26.000683500780724\n",
            "26.074398426047992\n",
            "26.08880584826693\n",
            "26.084280886316265\n",
            "25.99506202817429\n",
            "26.04798154020682\n",
            "26.130267452448606\n",
            "26.136782113462687\n",
            "26.133088121074252\n",
            "25.997350178018678\n",
            "26.057739910727832\n",
            "25.977678899056627\n",
            "26.166281981393695\n",
            "26.06530280224979\n",
            "26.057900768966647\n",
            "25.981791865138803\n",
            "26.001178632097435\n",
            "26.01308274373514\n",
            "26.017171046491057\n",
            "26.061858769971877\n",
            "25.985999520858968\n",
            "26.0686702048406\n",
            "26.03683804348111\n",
            "26.067267380654812\n",
            "26.0321219239853\n",
            "25.983623803078444\n",
            "26.108712962913387\n",
            "25.970299481124812\n",
            "26.045035183428553\n",
            "25.98714584220602\n",
            "25.986546919171815\n",
            "26.144929957126806\n",
            "26.08696526777021\n",
            "26.018184197365372\n",
            "26.00996976729948\n",
            "26.034797336444285\n",
            "26.05791124495431\n",
            "26.00895940454211\n",
            "26.05980677343905\n",
            "25.964312196319952\n",
            "26.016817157738842\n",
            "26.03477511297342\n",
            "26.069590955332387\n",
            "26.071218399505597\n",
            "26.012368700976367\n",
            "26.03358179423958\n",
            "25.92192676384002\n",
            "26.040762997465208\n",
            "26.069501110410783\n",
            "25.909274786245078\n",
            "25.974836916808272\n",
            "26.03411198174763\n",
            "26.060538163408637\n",
            "25.92593594948994\n",
            "26.067468749795808\n",
            "25.91641264272039\n",
            "26.01622274327383\n",
            "25.931613095104694\n",
            "25.918772169505246\n",
            "25.965763048938243\n",
            "26.009247927955585\n",
            "26.04822856670944\n",
            "26.01229780446738\n",
            "26.02872551434848\n",
            "26.001103160007915\n",
            "25.92808525777218\n",
            "25.96917347234026\n",
            "26.01473397384325\n",
            "26.00124192389194\n",
            "25.927183136926033\n",
            "25.87855429947922\n",
            "25.960713239365333\n",
            "25.977483288793337\n",
            "26.00216016569175\n",
            "25.986939423935837\n",
            "26.081178432828892\n",
            "26.0033939042587\n",
            "25.928685977647547\n",
            "26.01618086196686\n",
            "25.934655786608346\n",
            "25.973494358908283\n",
            "25.984748462680727\n",
            "25.950019977995453\n",
            "25.888541680993512\n",
            "25.948966907628346\n",
            "25.985834929157136\n",
            "26.00848080122887\n",
            "26.058830851145103\n",
            "25.95315056065374\n",
            "26.023382538813166\n",
            "25.996977653217073\n",
            "26.014175617252477\n",
            "25.987136207229923\n",
            "25.893817077274434\n",
            "26.052947260417568\n",
            "25.948527346830815\n",
            "25.928136648028158\n",
            "25.97880375641398\n",
            "25.96300110992979\n",
            "25.909426164114848\n",
            "26.067003641011524\n",
            "25.943002622256245\n",
            "26.012953576868313\n",
            "25.885795300535392\n",
            "26.037963648326695\n",
            "25.88649448734941\n",
            "25.971259623765945\n",
            "25.95375365304062\n",
            "25.92266699305037\n",
            "25.89614850754151\n",
            "25.963101447792724\n",
            "25.875129520893097\n",
            "25.979115330497734\n",
            "25.961310553364456\n",
            "25.968696904950775\n",
            "25.998576490208507\n",
            "25.891360760962574\n",
            "25.85118531889748\n",
            "25.994890563655645\n",
            "25.912667898344807\n",
            "25.982028504338814\n",
            "25.831924322410487\n",
            "25.89226210853667\n",
            "25.863136366009712\n",
            "25.851960470201448\n",
            "25.959423307213\n",
            "25.96186305199808\n",
            "25.866902048088377\n",
            "25.921944334419095\n",
            "25.93791340732423\n",
            "25.83913818449946\n",
            "25.947453352768207\n",
            "25.931922171572808\n",
            "25.949118251970503\n",
            "25.863139056877117\n",
            "25.920705867501965\n",
            "25.95548949915974\n",
            "26.00656434648954\n",
            "25.888888833997044\n",
            "25.907484757597558\n",
            "25.94508850830607\n",
            "25.949217647241312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kVlv7IX3RsY",
        "colab_type": "text"
      },
      "source": [
        "Use your trained model from the previous question to predict the sentiment score of words in the lyrics corpus that are not part of the original sentiment dataset. Review the words with the highest positive and negative sentiment. Do the results make sense?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgZUs39D3RsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PexZrC3Z3Rsb",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Word Vectors\n",
        "In this section, you'll plot words on a 2D grid based on their inner similarity. We'll use the tSNE transformation to reduce dimensions from 300 to 2. You can get sample code from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial or other tutorials online.\n",
        "\n",
        "Perform the following:\n",
        "- Keep only the 3,000 most frequent words (after removing stopwords)\n",
        "- For this list, compute for each word its relative abundance in each of the genres\n",
        "- Compute the ratio between the proportion of each word in each genre and the proportion of the word in the entire corpus (the background distribution)\n",
        "- Pick the top 50 words for each genre. These words give good indication for that genre. Join the words from all genres into a single list of top significant words. \n",
        "- Compute tSNE transformation to 2D for all words, based on their word vectors\n",
        "- Plot the list of the top significant words in 2D. Next to each word output its text. The color of each point should indicate the genre for which it is most significant.\n",
        "\n",
        "You might prefer to use a different number of points or a slightly different methodology for improved results.  \n",
        "Analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njRjVeOo3Rsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrrV48nU3Rsd",
        "colab_type": "text"
      },
      "source": [
        "## Text Classification\n",
        "In this section, you'll build a text classifier, determining the genre of a song based on its lyrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib5DLejw3Rsd",
        "colab_type": "text"
      },
      "source": [
        "### Text classification using Bag-of-Words\n",
        "Build a Naive Bayes classifier based on the bag of Words.  \n",
        "You will need to divide your dataset into a train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxmPDfzs3Rse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbGekJtg3Rsf",
        "colab_type": "text"
      },
      "source": [
        "Show the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YWleqZZ3Rsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mILK18503Rsh",
        "colab_type": "text"
      },
      "source": [
        "Show the classification report - precision, recall, f1 for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk6gn2V83Rsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMkFvrqg3Rsl",
        "colab_type": "text"
      },
      "source": [
        "### Text classification using Word Vectors\n",
        "#### Average word vectors\n",
        "Do the same, using a classifier that averages the word vectors of words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF4dN0lX3Rsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT37MjaM3Rsn",
        "colab_type": "text"
      },
      "source": [
        "#### TfIdf Weighting\n",
        "Do the same, using a classifier that averages the word vectors of words in the document, weighting each word by its TfIdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUbSsXYi3Rso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJJ0Zrx23Rsp",
        "colab_type": "text"
      },
      "source": [
        "### Text classification using ConvNet\n",
        "Do the same, using a ConvNet.  \n",
        "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
        "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
        "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
        "\n",
        "Extra: Try training the ConvNet with 2 slight modifications:\n",
        "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
        "1. random initialization of the embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01FKqgeL3Rsp",
        "colab_type": "text"
      },
      "source": [
        "You are encouraged to try this question on your own.  \n",
        "\n",
        "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
        "\n",
        "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxOIg8gc3Rsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}