{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw-lab-2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8FZ-mE8UYx0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0f83afa4-b387-4387-90c1-3451fd2b769a"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version\n",
        "\n",
        "%reset -f\n",
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "import sys\n",
        "sys.version\n",
        "\n",
        "#!pip3 install torch==0.4\n",
        "# !pip3 install torchvision\n",
        "\n",
        "!pip3 install 'torch==0.4.0'\n",
        "#!pip3 install 'torchvision==0.2.1'\n",
        "#!pip3 install --no-cache-dir -I 'pillow==5.1.0'\n",
        "#!pip3 install torchvision\n",
        "print('done')\n",
        "# Restart Kernel\n",
        "# This workaround is needed to properly upgrade PIL on Google Colab.\n",
        "import os\n",
        "os._exit(0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5b84e000 @  0x7fbe42aac2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VbiDUyaAYx0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cb51ac1f-c3bf-4268-fb15-3ae35a318396"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print(999)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "print('use_cuda = {0}'.format(use_cuda))\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random \n",
        "import numpy as np\n",
        "SEED = 999\n",
        "\n",
        "def fixSeed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "fixSeed(SEED)\n",
        "\n",
        "\n",
        "print(1111)\n",
        "\n",
        "class DatasetLoader(Dataset):\n",
        "\n",
        "    def __init__(self, xs, ys):\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    @staticmethod\n",
        "    def find_classes(root_dir):\n",
        "\n",
        "        train_dir = os.path.join(root_dir, 'train')\n",
        "\n",
        "        labels_folders = os.listdir(train_dir)\n",
        "\n",
        "        xs = []\n",
        "        ys = []\n",
        "        class_to_idx = {}\n",
        "        for index, label in enumerate(labels_folders):\n",
        "            class_to_idx[label] = index\n",
        "            y_train_folder = os.path.join(train_dir, label)\n",
        "            for file in os.listdir(y_train_folder):\n",
        "                train_example_path = os.path.join(y_train_folder, file)\n",
        "                xs.append([train_example_path, index, label])\n",
        "                ys.append(index)\n",
        "        return labels_folders, class_to_idx, len(labels_folders), xs, ys\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fullname = self.xs[idx][0]\n",
        "\n",
        "        image = Image.open(fullname).convert('RGB')\n",
        "        labels = self.ys[idx] \n",
        "\n",
        "        return image, labels\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__pyTorch VERSION: 0.4.0\n",
            "999\n",
            "use_cuda = False\n",
            "1111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0svANSQYx0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# labels_folders, class_to_idx,  num_of_labels, all_data_df = DatasetLoader.find_classes('all')\n",
        "# train_data = all_data_df.sample(frac=0.85)\n",
        "# valid_data = all_data_df[~all_data_df['file'].isin(train_data['file'])]\n",
        "# loader = DatasetLoader(train_df)\n",
        "\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8Fk-2SGYx0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.upload()\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "\n",
        "# from googleapiclient.discovery import build\n",
        "# drive_service = build('drive', 'v3')\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# labels_folders, class_to_idx,  num_of_labels, xs, ys = DatasetLoader.find_classes('all')\n",
        "# train_x, test_x, train_y, test_y = train_test_split(xs,ys, test_size=0.33, random_state=SEED)\n",
        "# loader = DatasetLoader(train_x, train_y)\n",
        "\n",
        "# for i in range(len(loader)):\n",
        "#     image, label = loader[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6eoSPgj7Yx0e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RWXgF_Hol2o8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_id = '17Cp4ZxCYGzWNypZo1WPiIz20x06xgPAt' # URL id. \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('shaurya.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "npOurGqVhKwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ea0729e3-a1fd-4a98-cae2-7836f71b826f"
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "ref                                                           title                                                size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "mehdidag/black-friday                                         Black Friday                                          5MB  2018-07-25 20:49:48          22569  \n",
            "spscientist/students-performance-in-exams                     Students Performance in Exams                         8KB  2018-11-09 18:25:25           7800  \n",
            "szamil/who-suicide-statistics                                 WHO Suicide Statistics                              307KB  2018-08-29 21:23:11           5698  \n",
            "lava18/google-play-store-apps                                 Google Play Store Apps                                2MB  2018-09-18 20:49:49          28276  \n",
            "nguyenhoc/plane-crash                                         Historical Plane Crashes                            700KB  2018-11-21 04:32:41           1241  \n",
            "iarunava/happy-house-dataset                                  Happy House Dataset                                   7MB  2018-09-07 06:33:10           1448  \n",
            "kaggle/kaggle-survey-2018                                     2018 Kaggle ML & DS Survey Challenge                  4MB  2018-11-03 22:35:07           6615  \n",
            "jessicali9530/kuc-hackathon-winter-2018                       UCI ML Drug Review dataset                           40MB  2018-11-12 19:44:19            386  \n",
            "jkkphys/english-wikipedia-articles-20170820-sqlite            English Wikipedia Articles 2017-08-20 SQLite          7GB  2018-11-27 21:54:22            161  \n",
            "jrobischon/wikipedia-movie-plots                              Wikipedia Movie Plots                                30MB  2018-10-15 19:59:54           1196  \n",
            "thedownhill/art-images-drawings-painting-sculpture-engraving  Art Images: Drawing/Painting/Sculptures/Engravings  581MB  2018-05-25 18:00:50            992  \n",
            "robikscube/hourly-energy-consumption                          Hourly Energy Consumption                            11MB  2018-08-30 14:17:03           2401  \n",
            "footprintnetwork/national-footprint-accounts-2018             National Footprint Accounts 2018                      3MB  2018-07-11 18:26:33           1003  \n",
            "sid321axn/amazon-alexa-reviews                                Amazon Alexa Reviews                                164KB  2018-07-31 17:45:14           1171  \n",
            "cityofLA/los-angeles-international-airport-data               Los Angeles International Airport Data              123KB  2018-12-25 22:05:22           1157  \n",
            "huffingtonpost/pollster-congressional-districts               Pollster Congressional Districts                    530KB  2018-11-07 04:49:42             93  \n",
            "olistbr/marketing-funnel-olist                                Marketing Funnel by Olist                           275KB  2018-11-16 14:00:20            346  \n",
            "danofer/sarcasm                                               Sarcasm on Reddit                                   216MB  2018-05-27 08:19:04           1340  \n",
            "septa97/100k-courseras-course-reviews-dataset                 100K Coursera's Course Reviews Dataset               12MB  2018-08-02 03:44:18            836  \n",
            "unitednations/un-general-debates                              UN General Debates                                   46MB  2017-09-05 23:12:36            788  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}